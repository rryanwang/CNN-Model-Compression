{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-eC-sb34T9w"
   },
   "source": [
    "## Accelerate Inference: Neural Network Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5PxMDZ-Z8gD",
    "outputId": "6575f092-4561-41d4-b85c-d472f7f85057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L47XBZWm4T9x",
    "outputId": "853ad040-0401-4f6d-e4f0-653dc75658fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1FQTVeAuNiU",
    "outputId": "d1ed07bb-6175-4567-e32b-e429459701e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.pkl\n",
      "train_labels.pkl\n",
      "val_images.pkl\n",
      "val_labels.pkl\n"
     ]
    }
   ],
   "source": [
    "# untar\n",
    "!tar -xvzf dataset.tar.gz\n",
    "# load train\n",
    "train_images = pickle.load(open('train_images.pkl', 'rb'))\n",
    "train_labels = pickle.load(open('train_labels.pkl', 'rb'))\n",
    "# load val\n",
    "val_images = pickle.load(open('val_images.pkl', 'rb'))\n",
    "val_labels = pickle.load(open('val_labels.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "KE9JuZDG4T94"
   },
   "outputs": [],
   "source": [
    "# Define the neural network architecture (don't change this)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTzcSoYl4T97",
    "outputId": "de80d0bd-7af9-4509-d3c1-509ab6fda440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_48 (Conv2D)          (None, 25, 25, 32)        896       \n",
      "                                                                 \n",
      " activation_78 (Activation)  (None, 25, 25, 32)        0         \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 23, 23, 32)        9248      \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 23, 23, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 11, 11, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 11, 11, 32)        0         \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " activation_80 (Activation)  (None, 11, 11, 64)        0         \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 9, 9, 64)          36928     \n",
      "                                                                 \n",
      " activation_81 (Activation)  (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " activation_82 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      " activation_83 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 592,933\n",
      "Trainable params: 592,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9Nk_MAPqZPt",
    "outputId": "10781ec7-72bf-4e5e-fc5a-258264253440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1.5304 - accuracy: 0.3023 - val_loss: 1.4099 - val_accuracy: 0.3735\n",
      "Epoch 2/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1.3552 - accuracy: 0.4141 - val_loss: 1.2658 - val_accuracy: 0.4677\n",
      "Epoch 3/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.2923 - accuracy: 0.4490 - val_loss: 1.2163 - val_accuracy: 0.4859\n",
      "Epoch 4/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.2386 - accuracy: 0.4848 - val_loss: 1.1732 - val_accuracy: 0.5073\n",
      "Epoch 5/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.2057 - accuracy: 0.5020 - val_loss: 1.1384 - val_accuracy: 0.5251\n",
      "Epoch 6/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1.1728 - accuracy: 0.5229 - val_loss: 1.1192 - val_accuracy: 0.5450\n",
      "Epoch 7/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.1415 - accuracy: 0.5389 - val_loss: 1.0809 - val_accuracy: 0.5517\n",
      "Epoch 8/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.1183 - accuracy: 0.5452 - val_loss: 1.0618 - val_accuracy: 0.5624\n",
      "Epoch 9/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.0917 - accuracy: 0.5592 - val_loss: 1.0512 - val_accuracy: 0.5679\n",
      "Epoch 10/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1.0675 - accuracy: 0.5759 - val_loss: 1.0216 - val_accuracy: 0.5850\n",
      "Epoch 11/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.0496 - accuracy: 0.5803 - val_loss: 0.9991 - val_accuracy: 0.5929\n",
      "Epoch 12/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 1.0295 - accuracy: 0.5898 - val_loss: 0.9872 - val_accuracy: 0.5984\n",
      "Epoch 13/50\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 1.0138 - accuracy: 0.5995 - val_loss: 0.9767 - val_accuracy: 0.6067\n",
      "Epoch 14/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.9986 - accuracy: 0.6079 - val_loss: 0.9599 - val_accuracy: 0.6032\n",
      "Epoch 15/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.9877 - accuracy: 0.6099 - val_loss: 1.0067 - val_accuracy: 0.5857\n",
      "Epoch 16/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.9650 - accuracy: 0.6210 - val_loss: 0.9446 - val_accuracy: 0.6194\n",
      "Epoch 17/50\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 0.9594 - accuracy: 0.6229 - val_loss: 0.9360 - val_accuracy: 0.6285\n",
      "Epoch 18/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.9404 - accuracy: 0.6331 - val_loss: 0.9674 - val_accuracy: 0.6059\n",
      "Epoch 19/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.9297 - accuracy: 0.6404 - val_loss: 0.9670 - val_accuracy: 0.6143\n",
      "Epoch 20/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.9188 - accuracy: 0.6439 - val_loss: 0.9047 - val_accuracy: 0.6317\n",
      "Epoch 21/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.9047 - accuracy: 0.6465 - val_loss: 0.8909 - val_accuracy: 0.6448\n",
      "Epoch 22/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.8963 - accuracy: 0.6499 - val_loss: 0.8872 - val_accuracy: 0.6511\n",
      "Epoch 23/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.8801 - accuracy: 0.6595 - val_loss: 0.8693 - val_accuracy: 0.6582\n",
      "Epoch 24/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.8692 - accuracy: 0.6638 - val_loss: 0.8873 - val_accuracy: 0.6428\n",
      "Epoch 25/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.8516 - accuracy: 0.6719 - val_loss: 0.8734 - val_accuracy: 0.6562\n",
      "Epoch 26/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.8440 - accuracy: 0.6723 - val_loss: 0.8740 - val_accuracy: 0.6523\n",
      "Epoch 27/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.8407 - accuracy: 0.6733 - val_loss: 0.8387 - val_accuracy: 0.6721\n",
      "Epoch 28/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.8236 - accuracy: 0.6824 - val_loss: 0.8320 - val_accuracy: 0.6705\n",
      "Epoch 29/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.8145 - accuracy: 0.6861 - val_loss: 0.8219 - val_accuracy: 0.6717\n",
      "Epoch 30/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.8014 - accuracy: 0.6887 - val_loss: 0.8212 - val_accuracy: 0.6764\n",
      "Epoch 31/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.7914 - accuracy: 0.6954 - val_loss: 0.7984 - val_accuracy: 0.6883\n",
      "Epoch 32/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.7811 - accuracy: 0.7001 - val_loss: 0.8202 - val_accuracy: 0.6804\n",
      "Epoch 33/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.7722 - accuracy: 0.7021 - val_loss: 0.8016 - val_accuracy: 0.6927\n",
      "Epoch 34/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.7685 - accuracy: 0.7057 - val_loss: 0.8091 - val_accuracy: 0.6867\n",
      "Epoch 35/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.7527 - accuracy: 0.7121 - val_loss: 0.7864 - val_accuracy: 0.6962\n",
      "Epoch 36/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.7456 - accuracy: 0.7185 - val_loss: 0.7839 - val_accuracy: 0.6907\n",
      "Epoch 37/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.7393 - accuracy: 0.7161 - val_loss: 0.7829 - val_accuracy: 0.6994\n",
      "Epoch 38/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.7228 - accuracy: 0.7263 - val_loss: 0.7634 - val_accuracy: 0.7073\n",
      "Epoch 39/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.7176 - accuracy: 0.7266 - val_loss: 0.8308 - val_accuracy: 0.6749\n",
      "Epoch 40/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.7138 - accuracy: 0.7293 - val_loss: 0.7564 - val_accuracy: 0.7061\n",
      "Epoch 41/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.7035 - accuracy: 0.7297 - val_loss: 0.7632 - val_accuracy: 0.7069\n",
      "Epoch 42/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.6989 - accuracy: 0.7350 - val_loss: 0.7628 - val_accuracy: 0.7053\n",
      "Epoch 43/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.6832 - accuracy: 0.7400 - val_loss: 0.7785 - val_accuracy: 0.7006\n",
      "Epoch 44/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.6845 - accuracy: 0.7443 - val_loss: 0.7437 - val_accuracy: 0.7141\n",
      "Epoch 45/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.6666 - accuracy: 0.7493 - val_loss: 0.7571 - val_accuracy: 0.7081\n",
      "Epoch 46/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.6659 - accuracy: 0.7491 - val_loss: 0.7490 - val_accuracy: 0.7168\n",
      "Epoch 47/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.6546 - accuracy: 0.7542 - val_loss: 0.7365 - val_accuracy: 0.7180\n",
      "Epoch 48/50\n",
      "703/703 [==============================] - 3s 5ms/step - loss: 0.6481 - accuracy: 0.7548 - val_loss: 0.7405 - val_accuracy: 0.7145\n",
      "Epoch 49/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.6367 - accuracy: 0.7590 - val_loss: 0.7287 - val_accuracy: 0.7255\n",
      "Epoch 50/50\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 0.6294 - accuracy: 0.7608 - val_loss: 0.7461 - val_accuracy: 0.7204\n"
     ]
    }
   ],
   "source": [
    "# you can use the default hyper-parameters for training, \n",
    "# and val accuracy ~59% after 25 epochs and > 63% after 50 epochs\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 50\n",
    "history = model.fit(train_images, train_labels, batch_size=32, epochs=epochs, \n",
    "                    validation_data=(val_images, val_labels)) # train for 50 epochs, with batch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOhpP7M24T9_",
    "outputId": "847e2d2a-eff7-4481-c9a9-c05cca600f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 5ms/step - loss: 0.7461 - accuracy: 0.7204\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(val_images, val_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "vjw94aij4T-C"
   },
   "outputs": [],
   "source": [
    "# perform pruning here\n",
    "\n",
    "# get the weights \n",
    "weights = model.get_weights()\n",
    "\n",
    "# you can use set_weights() to set some weights to zero, e.g.,\n",
    "# some references for pruning techniques: https://arxiv.org/pdf/1810.05270v2.pdf, https://arxiv.org/pdf/2001.04062.pdf\n",
    "\n",
    "# get the layers\n",
    "layers = model.layers\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "prune_rate = 30\n",
    "def compute_mask(weights):\n",
    "  shape = weights.shape\n",
    "  abs_weights = np.abs(weights)\n",
    "  # abs_weights = abs_weights.reshape((-1, shape[-1]))\n",
    "  abs_weights = abs_weights.reshape(-1)\n",
    "  threshold = np.percentile(abs_weights, prune_rate)\n",
    "  # threshold = tfp.stats.percentile(abs_weights, prune_rate)\n",
    "  abs_weights = abs_weights.reshape(shape)\n",
    "  # abs_weights = K.eval(abs_weights)\n",
    "  \n",
    "  mask = np.zeros(shape)\n",
    "  mask[abs_weights < threshold] = 1\n",
    "  # mask = mask.reshape(shape)\n",
    "  return mask\n",
    "\n",
    "constant = 0 # (will annulate kernel, but not bias) \n",
    "def stop_backprop(x, layers, i):\n",
    "  layer = layers[i]\n",
    "  weights0, weights1 = layer.weights\n",
    "\n",
    "  mask = compute_mask(weights0)\n",
    "  weights0 = weights0 * (1 - mask) + constant * mask\n",
    "\n",
    "  # stopped = K.stop_gradient(weights0)\n",
    "  # weights0 = weights0 * (1 - mask) + stopped * mask\n",
    "\n",
    "  layer.set_weights([weights0, weights1])\n",
    "  return layer(x)\n",
    "    \n",
    "pruned_model = models.Sequential()\n",
    "for i, layer in enumerate(layers):\n",
    "  # if len(layer.weights) > 0 and 'conv2d' in layer.name:\n",
    "  if len(layer.weights) > 0:\n",
    "    # print(len(layer.weights))\n",
    "    # print(layer.weights[0].shape)\n",
    "    # print(layer.weights[1].shape)\n",
    "    \n",
    "    new_layer = Lambda(stop_backprop, arguments={'layers': layers, 'i': i})\n",
    "    # new_layer.set_weights(layer.weights)\n",
    "    pruned_model.add(new_layer)\n",
    "  else:\n",
    "    pruned_model.add(layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVsaqbNdt1Az",
    "outputId": "dacaacd3-eaf2-4037-b904-166a392cf8ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_126), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_48/kernel:0' shape=(3, 3, 3, 32) dtype=float32, numpy=\n",
      "array([[[[-0.09391245, -0.12319858,  0.0630223 ,  0.05146995,\n",
      "           0.09712143, -0.06017947,  0.        , -0.1338746 ,\n",
      "           0.13758533,  0.        ,  0.12997559, -0.14218195,\n",
      "           0.        , -0.05573569, -0.06492909,  0.        ,\n",
      "           0.        ,  0.09718483,  0.        , -0.10073642,\n",
      "          -0.12685847,  0.        , -0.11324305, -0.07133701,\n",
      "           0.        ,  0.08169644,  0.09012291,  0.13352159,\n",
      "          -0.04998351,  0.05446361,  0.        , -0.14893429],\n",
      "         [-0.14182726, -0.09173274, -0.11326265,  0.10635749,\n",
      "           0.05046377,  0.        ,  0.        ,  0.        ,\n",
      "           0.06524117,  0.07906832,  0.        , -0.10011062,\n",
      "          -0.08106556, -0.0489588 ,  0.        ,  0.14082575,\n",
      "           0.1104186 ,  0.        ,  0.        ,  0.1030073 ,\n",
      "           0.13901807,  0.        ,  0.        , -0.05921436,\n",
      "          -0.16141662,  0.        ,  0.05942736,  0.14574562,\n",
      "           0.        ,  0.09916531,  0.07987568,  0.05403027],\n",
      "         [-0.11246721, -0.10742173,  0.17285503,  0.14948565,\n",
      "          -0.07539527,  0.        ,  0.10835872,  0.        ,\n",
      "          -0.04748211, -0.0834211 ,  0.05845201,  0.        ,\n",
      "           0.09391746, -0.13385646, -0.10717705,  0.0834443 ,\n",
      "           0.        ,  0.        , -0.07059934,  0.07788766,\n",
      "           0.        ,  0.        , -0.09930427,  0.11944195,\n",
      "           0.        ,  0.06980389, -0.11753336,  0.17248772,\n",
      "           0.        ,  0.12595803, -0.09893855, -0.11606337]],\n",
      "\n",
      "        [[ 0.1444348 ,  0.1494804 ,  0.        , -0.13352366,\n",
      "           0.        , -0.10007766,  0.14308502,  0.12496773,\n",
      "           0.07734001, -0.07838856,  0.11356205,  0.        ,\n",
      "           0.        ,  0.        ,  0.        ,  0.10461401,\n",
      "          -0.06488895, -0.0540421 , -0.13246195,  0.        ,\n",
      "          -0.07968111,  0.12645292,  0.        ,  0.        ,\n",
      "          -0.13194588,  0.06805065, -0.05949682,  0.11705183,\n",
      "           0.        ,  0.16781312,  0.1463598 ,  0.06605012],\n",
      "         [ 0.12249254,  0.15321188, -0.1156776 ,  0.        ,\n",
      "           0.05798746,  0.        ,  0.12178469,  0.        ,\n",
      "          -0.13793817,  0.12432507,  0.1159891 ,  0.        ,\n",
      "          -0.08125295, -0.07221879,  0.1192106 , -0.07821079,\n",
      "           0.12964   ,  0.11490912, -0.11738029,  0.06442966,\n",
      "           0.        , -0.08467445,  0.        ,  0.        ,\n",
      "           0.        , -0.13465771, -0.07130602,  0.        ,\n",
      "           0.        ,  0.1270475 , -0.05257904,  0.1460727 ],\n",
      "         [ 0.14240113,  0.15109213, -0.06688743, -0.06630953,\n",
      "           0.07209975, -0.1296803 ,  0.06977575, -0.11736059,\n",
      "           0.08139086,  0.        ,  0.        , -0.15471901,\n",
      "           0.08981153,  0.10808162,  0.10495221,  0.        ,\n",
      "           0.12378773,  0.07420956,  0.        ,  0.        ,\n",
      "           0.        ,  0.06744149, -0.10281323,  0.09975551,\n",
      "           0.10244397,  0.10933849,  0.11630673, -0.09219999,\n",
      "           0.1111961 ,  0.05854826,  0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ,  0.1592274 ,  0.        ,\n",
      "          -0.08103307, -0.06078638,  0.09730481,  0.08256295,\n",
      "          -0.04862687,  0.        ,  0.        ,  0.        ,\n",
      "           0.        ,  0.        ,  0.08491952,  0.        ,\n",
      "          -0.04832183, -0.11736985,  0.07224026,  0.07162975,\n",
      "          -0.07724395, -0.07162903,  0.1589798 , -0.12284213,\n",
      "           0.        ,  0.        ,  0.        , -0.07763465,\n",
      "           0.        ,  0.        , -0.08894842,  0.08464477],\n",
      "         [-0.14537641,  0.05631851,  0.        , -0.15728356,\n",
      "           0.        ,  0.        , -0.08198547,  0.        ,\n",
      "          -0.09572338, -0.04868959,  0.        ,  0.11377693,\n",
      "          -0.13268511,  0.        ,  0.05908708, -0.12406375,\n",
      "           0.05947957, -0.12688187, -0.09639294, -0.06386484,\n",
      "           0.06648393, -0.16930687, -0.07014516,  0.        ,\n",
      "          -0.06980634, -0.11253352,  0.09514634,  0.10956367,\n",
      "          -0.08800849,  0.        , -0.09080084,  0.        ],\n",
      "         [ 0.        , -0.0975898 , -0.0735333 , -0.08264774,\n",
      "           0.12057924,  0.1436096 , -0.12101819,  0.13231964,\n",
      "           0.        , -0.11156993,  0.        , -0.08354883,\n",
      "          -0.07143968,  0.0780048 ,  0.05967491, -0.14024411,\n",
      "           0.1417854 ,  0.        , -0.14965715, -0.07606209,\n",
      "           0.        , -0.11723159,  0.07147818,  0.        ,\n",
      "           0.        ,  0.08947445,  0.07905466, -0.05173426,\n",
      "          -0.09987753, -0.13081639, -0.08948269, -0.14402384]]],\n",
      "\n",
      "\n",
      "       [[[ 0.        , -0.17022812, -0.10273254,  0.09041643,\n",
      "          -0.06552051,  0.10168367, -0.13331   , -0.13488749,\n",
      "          -0.11057508,  0.12212213, -0.0771073 , -0.14209694,\n",
      "          -0.10461447,  0.06764703,  0.13485403,  0.        ,\n",
      "          -0.06060277, -0.08026597,  0.05022419, -0.11619295,\n",
      "          -0.06575099,  0.08851201, -0.07720479,  0.        ,\n",
      "           0.04757137,  0.0655794 , -0.12779473, -0.11818843,\n",
      "          -0.13066787,  0.09359086,  0.128951  , -0.10429174],\n",
      "         [ 0.        ,  0.        ,  0.06828288,  0.12662922,\n",
      "           0.08080854, -0.0701733 , -0.10700261, -0.09097013,\n",
      "           0.        ,  0.13719217,  0.0702257 , -0.09518619,\n",
      "          -0.04907839,  0.08152574, -0.07919119,  0.        ,\n",
      "          -0.04702255,  0.0753174 ,  0.        ,  0.        ,\n",
      "           0.        , -0.12446232,  0.05392775, -0.05722745,\n",
      "          -0.11315719, -0.13917801, -0.04996459,  0.09722121,\n",
      "           0.10455745, -0.11956581,  0.15668057,  0.        ],\n",
      "         [-0.05954102,  0.        ,  0.15347533, -0.09414535,\n",
      "           0.        ,  0.        ,  0.        ,  0.        ,\n",
      "           0.        ,  0.        , -0.05469684,  0.04685324,\n",
      "          -0.11193782,  0.        ,  0.09965611,  0.12789473,\n",
      "           0.12341671,  0.        ,  0.14965014,  0.1054123 ,\n",
      "           0.        , -0.07776422,  0.13160275,  0.1323273 ,\n",
      "          -0.13852435,  0.1539655 ,  0.        ,  0.        ,\n",
      "           0.        , -0.12097601,  0.        , -0.13046111]],\n",
      "\n",
      "        [[ 0.10329914,  0.        ,  0.        , -0.06278905,\n",
      "           0.        ,  0.        ,  0.11277905,  0.14684175,\n",
      "           0.0794503 , -0.0475762 , -0.06543887, -0.14077815,\n",
      "           0.09088329,  0.        ,  0.        , -0.05598514,\n",
      "           0.        ,  0.        , -0.08870976,  0.        ,\n",
      "          -0.0736746 ,  0.09872082,  0.        , -0.07080203,\n",
      "           0.12071872,  0.        , -0.10854455, -0.09705136,\n",
      "           0.05662719, -0.06556886, -0.08210438, -0.07737418],\n",
      "         [ 0.19713818,  0.        , -0.13266227,  0.07549604,\n",
      "           0.        ,  0.10268119,  0.14893258,  0.        ,\n",
      "          -0.06090834,  0.13963212, -0.11277082, -0.05581501,\n",
      "           0.06498353, -0.0734136 ,  0.        ,  0.0648793 ,\n",
      "          -0.07365418,  0.05519526,  0.        ,  0.07245652,\n",
      "           0.        ,  0.10896961,  0.08348444,  0.        ,\n",
      "          -0.06658401,  0.        ,  0.1109958 ,  0.0639734 ,\n",
      "           0.        , -0.13944323,  0.        ,  0.        ],\n",
      "         [ 0.1150089 ,  0.06899806, -0.05076734,  0.        ,\n",
      "          -0.11293682, -0.1096718 , -0.1015585 ,  0.12536167,\n",
      "          -0.10664657, -0.11238145,  0.        , -0.12595998,\n",
      "           0.        , -0.10220222, -0.12413924, -0.12919001,\n",
      "           0.14482407,  0.        ,  0.        ,  0.06559171,\n",
      "           0.        ,  0.09113145,  0.        ,  0.14431788,\n",
      "           0.11658446,  0.10398954, -0.05804119, -0.08775134,\n",
      "          -0.0866129 ,  0.09734795,  0.05003425,  0.        ]],\n",
      "\n",
      "        [[ 0.0818095 ,  0.        ,  0.        , -0.14610012,\n",
      "           0.09722556, -0.04822554,  0.1407015 ,  0.05512189,\n",
      "           0.        ,  0.        ,  0.05805709,  0.        ,\n",
      "          -0.13602768,  0.10519128,  0.        ,  0.08153488,\n",
      "          -0.11653193, -0.09148356,  0.05883835, -0.09681241,\n",
      "          -0.12050388,  0.07703953,  0.        , -0.11480989,\n",
      "           0.1377905 ,  0.        , -0.06524202,  0.        ,\n",
      "           0.04649621,  0.07587916, -0.0995835 ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        , -0.12857938,\n",
      "          -0.12323652,  0.        , -0.10517219, -0.11704809,\n",
      "           0.06085692, -0.11449251,  0.        ,  0.12678808,\n",
      "           0.12239704,  0.06003001, -0.11869278,  0.14682297,\n",
      "           0.0473324 ,  0.0642941 , -0.11748967,  0.10777134,\n",
      "           0.        ,  0.06277209,  0.05398325,  0.13104168,\n",
      "          -0.08106259,  0.11208832,  0.        , -0.14569692,\n",
      "           0.10334674,  0.06290865,  0.06645622,  0.14852159],\n",
      "         [-0.06857044,  0.07964788,  0.        , -0.12130257,\n",
      "          -0.1047579 ,  0.        , -0.13967653,  0.        ,\n",
      "           0.        ,  0.        ,  0.1232501 ,  0.11523385,\n",
      "          -0.05949318,  0.07614265,  0.        , -0.13639611,\n",
      "           0.11244105,  0.05395731, -0.0666571 , -0.07198982,\n",
      "          -0.0655074 ,  0.        ,  0.07961939,  0.        ,\n",
      "           0.        , -0.09223451,  0.12403616,  0.05846934,\n",
      "          -0.06467976, -0.08429248,  0.0502605 ,  0.07805614]]],\n",
      "\n",
      "\n",
      "       [[[ 0.        ,  0.08851369,  0.        , -0.06188478,\n",
      "           0.1337291 ,  0.13339841,  0.12603621,  0.07636829,\n",
      "           0.04708821, -0.11067639,  0.        ,  0.        ,\n",
      "           0.        ,  0.10324553,  0.        , -0.07862342,\n",
      "          -0.11411645,  0.        ,  0.04768056,  0.        ,\n",
      "           0.        ,  0.        ,  0.        ,  0.04640479,\n",
      "           0.        ,  0.08483247,  0.07048722, -0.05147589,\n",
      "           0.        , -0.05387134,  0.        ,  0.        ],\n",
      "         [ 0.12956426,  0.11652851,  0.        ,  0.10416484,\n",
      "           0.        ,  0.12638612,  0.14377415,  0.        ,\n",
      "          -0.10497593,  0.068516  ,  0.        ,  0.        ,\n",
      "           0.06570135,  0.        ,  0.08820628,  0.        ,\n",
      "           0.05865159, -0.12003785,  0.07666399,  0.05196077,\n",
      "          -0.06447475,  0.10279567, -0.1492719 , -0.09430066,\n",
      "          -0.12447049, -0.06786215,  0.08052009, -0.0685981 ,\n",
      "          -0.09847469, -0.04650979,  0.14755145,  0.14901441],\n",
      "         [-0.05115734,  0.        ,  0.05560675, -0.06107375,\n",
      "          -0.07138697,  0.14229403,  0.        , -0.11370865,\n",
      "           0.        ,  0.12999544, -0.05088282,  0.05816044,\n",
      "          -0.07712179,  0.08370034,  0.        ,  0.105184  ,\n",
      "          -0.06170866,  0.        ,  0.0805852 , -0.07003988,\n",
      "           0.        ,  0.        , -0.0520758 , -0.05833003,\n",
      "           0.12066884,  0.07130996,  0.14900047, -0.09010188,\n",
      "           0.10715973, -0.05930162,  0.10247698,  0.05494459]],\n",
      "\n",
      "        [[-0.13251597, -0.09298285,  0.0870792 ,  0.18640026,\n",
      "          -0.06351137,  0.12008526, -0.08327381,  0.11218418,\n",
      "           0.17317502,  0.08281492,  0.10575672,  0.0751005 ,\n",
      "           0.        ,  0.        , -0.0930362 ,  0.        ,\n",
      "           0.13044675,  0.        ,  0.        ,  0.11050899,\n",
      "           0.        ,  0.10151297,  0.15063435,  0.        ,\n",
      "          -0.07433057, -0.07355122, -0.10743278, -0.13634771,\n",
      "           0.09685349, -0.0591528 ,  0.        ,  0.        ],\n",
      "         [-0.13880093,  0.        ,  0.        ,  0.18507396,\n",
      "           0.        , -0.11239348, -0.14836039, -0.14629343,\n",
      "           0.        ,  0.08534382,  0.        ,  0.08466035,\n",
      "           0.14650193,  0.05932019, -0.1122459 ,  0.        ,\n",
      "           0.        ,  0.09747693,  0.07557386,  0.13290451,\n",
      "          -0.05310084,  0.10788534,  0.15566455,  0.        ,\n",
      "           0.        ,  0.        , -0.0497403 ,  0.05155471,\n",
      "           0.        ,  0.06074869,  0.        ,  0.1220287 ],\n",
      "         [ 0.        ,  0.09201283,  0.11666827,  0.14430329,\n",
      "          -0.1421564 ,  0.14386074, -0.13418631,  0.        ,\n",
      "           0.12589742,  0.        ,  0.12269469, -0.06390377,\n",
      "          -0.04978282,  0.09132291,  0.0786996 , -0.09718005,\n",
      "           0.0945536 ,  0.09187646,  0.        ,  0.14615053,\n",
      "           0.        , -0.17353228,  0.        , -0.06422376,\n",
      "           0.09768106, -0.06397586,  0.13908832,  0.08738799,\n",
      "          -0.09046801, -0.11884305,  0.        , -0.13222148]],\n",
      "\n",
      "        [[ 0.        , -0.17015661,  0.        , -0.08387946,\n",
      "           0.08215797,  0.09090676,  0.1194725 ,  0.10605505,\n",
      "           0.05689248, -0.12363426, -0.06967685,  0.        ,\n",
      "           0.08508879,  0.        ,  0.        ,  0.        ,\n",
      "          -0.09637742, -0.05579933,  0.        , -0.05213776,\n",
      "           0.        ,  0.094249  ,  0.        ,  0.        ,\n",
      "           0.        ,  0.        ,  0.        ,  0.11468046,\n",
      "           0.12575424,  0.        , -0.10556324,  0.        ],\n",
      "         [ 0.        ,  0.12503412, -0.09070631,  0.06578182,\n",
      "           0.11289435, -0.13121748,  0.        , -0.13254419,\n",
      "          -0.12148359, -0.14559007, -0.1811443 ,  0.        ,\n",
      "           0.13364445,  0.11172706, -0.06388453, -0.11271729,\n",
      "          -0.09568225,  0.10520109,  0.06367853, -0.12575571,\n",
      "           0.        , -0.1467782 ,  0.        ,  0.11012973,\n",
      "           0.0784504 , -0.09055039, -0.13884969, -0.05099022,\n",
      "           0.        ,  0.0719346 , -0.1131525 , -0.0564554 ],\n",
      "         [-0.05587729,  0.04924139,  0.        ,  0.08523833,\n",
      "          -0.09881441, -0.11195721,  0.        ,  0.        ,\n",
      "          -0.06053301,  0.        , -0.07972345,  0.13292435,\n",
      "           0.        , -0.06283993,  0.        ,  0.        ,\n",
      "          -0.0854784 ,  0.14526469,  0.16252108,  0.        ,\n",
      "           0.        ,  0.07845757, -0.088851  , -0.13530134,\n",
      "           0.        ,  0.        ,  0.        , -0.12223238,\n",
      "           0.        ,  0.16881019, -0.0893324 ,  0.1166522 ]]]],\n",
      "      dtype=float32)>\n",
      "  <tf.Variable 'conv2d_48/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 0.03116559,  0.00898765,  0.00804852,  0.01375458,  0.0175474 ,\n",
      "        0.01234519,  0.00918908,  0.00359299, -0.00424046, -0.00070014,\n",
      "       -0.00387531,  0.0256726 ,  0.02685729, -0.05160178,  0.03966175,\n",
      "        0.01768514,  0.00338082, -0.03424263,  0.01722977, -0.00320188,\n",
      "        0.0505438 ,  0.0136588 , -0.00341319,  0.01426216,  0.0473972 ,\n",
      "       -0.01383938,  0.04338337,  0.0120067 , -0.00256025, -0.01562774,\n",
      "        0.02621863,  0.01927513], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_127), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_49/kernel:0' shape=(3, 3, 32, 32) dtype=float32, numpy=\n",
      "array([[[[ 0.06763635,  0.1442568 ,  0.        , ...,  0.0667038 ,\n",
      "          -0.03946861,  0.        ],\n",
      "         [ 0.06395826,  0.09294876,  0.        , ...,  0.10292845,\n",
      "           0.        , -0.05290403],\n",
      "         [ 0.14429016,  0.        ,  0.        , ...,  0.07422943,\n",
      "           0.05552283,  0.08575312],\n",
      "         ...,\n",
      "         [-0.0467681 ,  0.0757932 , -0.08914459, ...,  0.        ,\n",
      "           0.08862283,  0.11459967],\n",
      "         [-0.07044809,  0.15478575, -0.04614216, ...,  0.07348657,\n",
      "           0.        ,  0.        ],\n",
      "         [-0.09147138, -0.03301726,  0.        , ...,  0.        ,\n",
      "           0.        ,  0.07683094]],\n",
      "\n",
      "        [[ 0.04362596, -0.07192737, -0.05465691, ...,  0.12477305,\n",
      "           0.03533335,  0.09168591],\n",
      "         [ 0.05256804,  0.        , -0.09808915, ...,  0.05847315,\n",
      "           0.03644776,  0.09824134],\n",
      "         [ 0.        ,  0.        ,  0.06483392, ..., -0.09283418,\n",
      "           0.03828114, -0.05401308],\n",
      "         ...,\n",
      "         [ 0.        ,  0.04459944, -0.08170874, ...,  0.13253716,\n",
      "          -0.10247495,  0.        ],\n",
      "         [ 0.        ,  0.17487994,  0.        , ..., -0.05189469,\n",
      "           0.        , -0.13743313],\n",
      "         [ 0.07295859, -0.0510661 , -0.12629223, ...,  0.        ,\n",
      "           0.05813224,  0.        ]],\n",
      "\n",
      "        [[ 0.07813051,  0.        ,  0.        , ...,  0.        ,\n",
      "           0.07755785,  0.06281178],\n",
      "         [ 0.        ,  0.04584375, -0.08002236, ..., -0.12303367,\n",
      "           0.09242951,  0.        ],\n",
      "         [ 0.04821824,  0.        ,  0.07399218, ...,  0.05895582,\n",
      "           0.0833184 ,  0.1152119 ],\n",
      "         ...,\n",
      "         [ 0.        , -0.03589841, -0.10129643, ..., -0.08841725,\n",
      "          -0.05105127, -0.03511715],\n",
      "         [ 0.        , -0.11837516,  0.        , ..., -0.05291437,\n",
      "           0.        , -0.08705443],\n",
      "         [-0.07151312,  0.        ,  0.09622314, ...,  0.        ,\n",
      "          -0.10604019, -0.10696433]]],\n",
      "\n",
      "\n",
      "       [[[ 0.12166706,  0.        , -0.0522485 , ...,  0.        ,\n",
      "           0.06191275, -0.0748018 ],\n",
      "         [ 0.        ,  0.        ,  0.        , ..., -0.09149246,\n",
      "          -0.09200731, -0.08977769],\n",
      "         [ 0.10733791,  0.05793275,  0.        , ...,  0.08698503,\n",
      "           0.05234364, -0.06184808],\n",
      "         ...,\n",
      "         [ 0.        ,  0.        ,  0.        , ..., -0.04086551,\n",
      "          -0.10737744,  0.08112767],\n",
      "         [ 0.08791649,  0.13142926,  0.        , ..., -0.04250924,\n",
      "           0.        ,  0.09914924],\n",
      "         [-0.07315741,  0.        , -0.03261323, ..., -0.06350458,\n",
      "          -0.11137082,  0.05710172]],\n",
      "\n",
      "        [[ 0.11530466, -0.17854872,  0.        , ..., -0.14069557,\n",
      "           0.        ,  0.        ],\n",
      "         [ 0.03513873, -0.05250348,  0.06730418, ..., -0.03061843,\n",
      "          -0.0584007 , -0.03856101],\n",
      "         [ 0.        ,  0.12412874,  0.03820594, ..., -0.03515586,\n",
      "          -0.08213132,  0.04227045],\n",
      "         ...,\n",
      "         [ 0.        ,  0.05071986,  0.03270743, ..., -0.05878535,\n",
      "          -0.03446257,  0.        ],\n",
      "         [-0.03662557,  0.10584434,  0.05980359, ...,  0.0525641 ,\n",
      "           0.04956627,  0.        ],\n",
      "         [ 0.07635447, -0.12353335, -0.08931444, ...,  0.06485348,\n",
      "          -0.06861531,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        , -0.09129929, ...,  0.03208556,\n",
      "           0.        ,  0.05154268],\n",
      "         [ 0.        ,  0.06378274, -0.12256334, ...,  0.08868099,\n",
      "           0.04511906,  0.03059616],\n",
      "         [ 0.08302946, -0.08286985,  0.04023454, ...,  0.        ,\n",
      "          -0.07797949,  0.        ],\n",
      "         ...,\n",
      "         [ 0.        , -0.0502384 , -0.07969849, ...,  0.13046382,\n",
      "           0.06050291,  0.12066624],\n",
      "         [ 0.        ,  0.        ,  0.13021976, ..., -0.07184429,\n",
      "           0.        , -0.07011061],\n",
      "         [-0.05180316, -0.04291388,  0.04356166, ...,  0.03203822,\n",
      "          -0.05783183,  0.        ]]],\n",
      "\n",
      "\n",
      "       [[[-0.03694287,  0.08335774,  0.05999286, ...,  0.        ,\n",
      "           0.0802082 , -0.07313541],\n",
      "         [ 0.        ,  0.        ,  0.10051337, ...,  0.        ,\n",
      "           0.04441282, -0.10174035],\n",
      "         [ 0.10176549, -0.07496013,  0.04254972, ...,  0.03679137,\n",
      "          -0.03545444,  0.04598277],\n",
      "         ...,\n",
      "         [ 0.12603621,  0.        ,  0.08546647, ...,  0.10071919,\n",
      "           0.        , -0.12044889],\n",
      "         [ 0.08697661,  0.08539555,  0.        , ..., -0.05643456,\n",
      "          -0.04751372,  0.13253732],\n",
      "         [-0.12250739,  0.        ,  0.        , ...,  0.08606105,\n",
      "          -0.10299399,  0.        ]],\n",
      "\n",
      "        [[ 0.09050842, -0.15133199,  0.05273297, ..., -0.07966936,\n",
      "          -0.14749996, -0.04693697],\n",
      "         [ 0.05253552, -0.16376421,  0.04921199, ..., -0.1221027 ,\n",
      "          -0.04079999,  0.        ],\n",
      "         [ 0.09377325,  0.        ,  0.11510092, ...,  0.0399858 ,\n",
      "           0.03672827, -0.09414422],\n",
      "         ...,\n",
      "         [ 0.        ,  0.04447486, -0.07516173, ..., -0.07210382,\n",
      "           0.12852553, -0.03332499],\n",
      "         [ 0.05788262,  0.14054367,  0.07031959, ..., -0.13185193,\n",
      "          -0.08395354,  0.08402439],\n",
      "         [-0.05762564, -0.14730275, -0.09657731, ...,  0.        ,\n",
      "           0.        , -0.04161341]],\n",
      "\n",
      "        [[ 0.04705069,  0.04843234, -0.12294132, ..., -0.04831114,\n",
      "           0.        ,  0.15798189],\n",
      "         [ 0.        ,  0.10897683, -0.14224902, ..., -0.06377742,\n",
      "          -0.08875976, -0.07924003],\n",
      "         [ 0.14340036, -0.08158778,  0.03846617, ..., -0.04100487,\n",
      "          -0.0443579 , -0.04751024],\n",
      "         ...,\n",
      "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "           0.12189778, -0.03883599],\n",
      "         [ 0.03684923,  0.        ,  0.        , ...,  0.05479554,\n",
      "          -0.07286719,  0.        ],\n",
      "         [ 0.        ,  0.07746955,  0.        , ...,  0.09164088,\n",
      "          -0.14372344,  0.        ]]]], dtype=float32)>\n",
      "  <tf.Variable 'conv2d_49/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 0.01837455,  0.00829081, -0.0021853 , -0.01283742,  0.01064548,\n",
      "       -0.03067511, -0.00245528,  0.01668657,  0.01137314, -0.00617734,\n",
      "        0.002513  , -0.02528636, -0.01072406,  0.0443188 , -0.0074966 ,\n",
      "       -0.02645613, -0.00273116,  0.02708811,  0.01376775,  0.00137192,\n",
      "       -0.05607538,  0.01468439,  0.00084619,  0.02611701, -0.00481188,\n",
      "        0.01940513,  0.00311322, -0.02614499, -0.01344435,  0.00335387,\n",
      "       -0.03945791,  0.01869529], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_128), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_50/kernel:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\n",
      "array([[[[ 0.03378512,  0.03876055, -0.08643769, ...,  0.03085363,\n",
      "           0.07810111,  0.        ],\n",
      "         [ 0.        ,  0.12097003, -0.04307592, ...,  0.        ,\n",
      "          -0.04405148, -0.05166508],\n",
      "         [ 0.        ,  0.        ,  0.06293971, ...,  0.03549126,\n",
      "          -0.05392355, -0.03277954],\n",
      "         ...,\n",
      "         [-0.041585  ,  0.05457244,  0.07633996, ..., -0.0300818 ,\n",
      "           0.07795899,  0.        ],\n",
      "         [ 0.10037916, -0.05866224, -0.04565078, ...,  0.        ,\n",
      "          -0.10842776,  0.06566996],\n",
      "         [-0.06919533,  0.        ,  0.        , ...,  0.06581025,\n",
      "           0.        , -0.03206869]],\n",
      "\n",
      "        [[ 0.05411005, -0.11674409, -0.12291903, ...,  0.06254622,\n",
      "           0.03479389,  0.04003354],\n",
      "         [ 0.        ,  0.        , -0.09045098, ...,  0.05411839,\n",
      "           0.059135  , -0.04214083],\n",
      "         [ 0.08265182,  0.        , -0.09241351, ..., -0.02736732,\n",
      "           0.        ,  0.02844287],\n",
      "         ...,\n",
      "         [-0.06638192, -0.05799567,  0.06353758, ...,  0.02932567,\n",
      "           0.06619665,  0.09753258],\n",
      "         [ 0.        , -0.12479554,  0.03579664, ..., -0.02671808,\n",
      "          -0.04246929, -0.02838838],\n",
      "         [ 0.        , -0.11346547,  0.04180237, ...,  0.08676777,\n",
      "           0.07655335,  0.06685254]],\n",
      "\n",
      "        [[ 0.        , -0.1419057 , -0.04760422, ...,  0.04912489,\n",
      "          -0.09206105,  0.1141117 ],\n",
      "         [-0.10271897,  0.15921783, -0.10237406, ...,  0.        ,\n",
      "           0.12035999,  0.05713545],\n",
      "         [ 0.06118636, -0.06800107, -0.04608051, ...,  0.        ,\n",
      "           0.0491475 ,  0.06230422],\n",
      "         ...,\n",
      "         [ 0.06663799,  0.042007  ,  0.03422471, ...,  0.        ,\n",
      "           0.        ,  0.06988174],\n",
      "         [ 0.        , -0.16384824,  0.07777691, ...,  0.        ,\n",
      "          -0.06806167,  0.        ],\n",
      "         [ 0.        , -0.05979266, -0.0545696 , ..., -0.04312011,\n",
      "           0.        ,  0.11070766]]],\n",
      "\n",
      "\n",
      "       [[[ 0.        , -0.08069539,  0.        , ...,  0.        ,\n",
      "           0.        ,  0.03790927],\n",
      "         [-0.05381363,  0.07591631,  0.        , ...,  0.        ,\n",
      "          -0.0352454 ,  0.        ],\n",
      "         [ 0.        ,  0.06998686,  0.        , ...,  0.        ,\n",
      "          -0.07113089,  0.        ],\n",
      "         ...,\n",
      "         [ 0.09660327,  0.        ,  0.        , ...,  0.        ,\n",
      "           0.03913393, -0.07199482],\n",
      "         [ 0.10603668, -0.11494334, -0.03855763, ...,  0.        ,\n",
      "           0.        ,  0.05631648],\n",
      "         [ 0.03082911,  0.        ,  0.0911772 , ...,  0.04756207,\n",
      "           0.04609961,  0.        ]],\n",
      "\n",
      "        [[ 0.07542189, -0.10397653,  0.        , ..., -0.06136063,\n",
      "           0.07872097,  0.        ],\n",
      "         [-0.06323256, -0.0274929 , -0.08510174, ...,  0.        ,\n",
      "           0.07152639,  0.        ],\n",
      "         [ 0.        ,  0.06298509,  0.07243267, ..., -0.05009279,\n",
      "           0.        ,  0.05542022],\n",
      "         ...,\n",
      "         [ 0.08247098,  0.03500623,  0.05575038, ...,  0.09702518,\n",
      "           0.12553388,  0.        ],\n",
      "         [ 0.        , -0.0721807 , -0.04318536, ..., -0.05443902,\n",
      "          -0.04250761, -0.07570863],\n",
      "         [ 0.        ,  0.        ,  0.07752478, ...,  0.        ,\n",
      "           0.0820632 ,  0.        ]],\n",
      "\n",
      "        [[-0.03487626, -0.031916  ,  0.05602035, ...,  0.08556022,\n",
      "           0.        ,  0.06696944],\n",
      "         [-0.05342454,  0.08569527, -0.08884869, ..., -0.08542638,\n",
      "           0.06268945, -0.06666555],\n",
      "         [ 0.06124159,  0.02986349,  0.        , ..., -0.08513241,\n",
      "           0.0477448 ,  0.10060602],\n",
      "         ...,\n",
      "         [ 0.04188247,  0.0270923 ,  0.        , ...,  0.08562355,\n",
      "           0.13563494,  0.0923202 ],\n",
      "         [-0.06662171,  0.        ,  0.        , ...,  0.06931409,\n",
      "           0.        ,  0.        ],\n",
      "         [ 0.        , -0.10207259,  0.12190053, ...,  0.07331617,\n",
      "           0.07030061,  0.03936674]]],\n",
      "\n",
      "\n",
      "       [[[ 0.10433841,  0.02708213,  0.08461793, ...,  0.        ,\n",
      "           0.02891179,  0.08852606],\n",
      "         [ 0.        ,  0.06947584,  0.03249861, ..., -0.07637596,\n",
      "          -0.07810021, -0.04268537],\n",
      "         [ 0.        ,  0.05524811,  0.        , ...,  0.0272015 ,\n",
      "          -0.03080571, -0.03307863],\n",
      "         ...,\n",
      "         [ 0.11092499,  0.        ,  0.12267559, ...,  0.09710532,\n",
      "           0.        ,  0.        ],\n",
      "         [ 0.        , -0.09334423,  0.        , ..., -0.03904019,\n",
      "          -0.0917479 , -0.04189216],\n",
      "         [ 0.0532225 ,  0.02884093,  0.10859313, ..., -0.03965201,\n",
      "           0.11386254,  0.        ]],\n",
      "\n",
      "        [[-0.06959846, -0.06009011, -0.05162352, ...,  0.07040614,\n",
      "           0.066301  ,  0.        ],\n",
      "         [-0.038608  ,  0.06255587,  0.        , ..., -0.10938538,\n",
      "           0.        , -0.04439064],\n",
      "         [ 0.03818146,  0.04631122,  0.03739398, ...,  0.03100546,\n",
      "          -0.05726001, -0.09637993],\n",
      "         ...,\n",
      "         [ 0.        ,  0.08555957,  0.        , ...,  0.12498681,\n",
      "           0.10955111, -0.04025665],\n",
      "         [ 0.03503972, -0.07760806,  0.09984504, ...,  0.09711154,\n",
      "          -0.03421659,  0.05544792],\n",
      "         [ 0.05537934,  0.05695605,  0.11712107, ...,  0.0733133 ,\n",
      "           0.14587212, -0.0370058 ]],\n",
      "\n",
      "        [[-0.09182094, -0.11361302,  0.07089397, ...,  0.08857333,\n",
      "           0.05861939, -0.03635518],\n",
      "         [ 0.        ,  0.        ,  0.05697784, ..., -0.12717411,\n",
      "           0.08142142, -0.12857981],\n",
      "         [ 0.        , -0.03003751,  0.03301382, ..., -0.08946328,\n",
      "           0.        ,  0.07728526],\n",
      "         ...,\n",
      "         [ 0.        ,  0.07159873,  0.07757763, ...,  0.0565465 ,\n",
      "           0.07692222,  0.07361344],\n",
      "         [ 0.08057918,  0.        ,  0.09555078, ...,  0.11523884,\n",
      "           0.        ,  0.04077639],\n",
      "         [ 0.        ,  0.        ,  0.10420406, ...,  0.        ,\n",
      "           0.09470392,  0.06059574]]]], dtype=float32)>\n",
      "  <tf.Variable 'conv2d_50/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([ 0.00787755,  0.01849774,  0.00132994,  0.03363292,  0.01216043,\n",
      "        0.02382017, -0.00306573,  0.00307252, -0.0274625 , -0.03063104,\n",
      "        0.05601973,  0.01686332,  0.0492874 , -0.02771075,  0.03291373,\n",
      "       -0.00364109, -0.01081919,  0.05223029,  0.07133157,  0.02698465,\n",
      "        0.03930145,  0.01716666,  0.01588565,  0.00529771,  0.01879462,\n",
      "        0.00261728,  0.05001992,  0.04353867, -0.05376551,  0.04970929,\n",
      "        0.05553051, -0.00867192,  0.03272029,  0.05882266,  0.01658529,\n",
      "        0.05031713, -0.05299032,  0.03428623,  0.05193567, -0.0001756 ,\n",
      "        0.02704206,  0.00553638,  0.00488008,  0.01934071,  0.04786138,\n",
      "        0.03689082,  0.00759803, -0.00095763, -0.05632101,  0.01794201,\n",
      "       -0.00266592, -0.06119584,  0.00348793, -0.02328013,  0.03355978,\n",
      "        0.03544064, -0.00092859,  0.0116916 ,  0.04639061,  0.01443818,\n",
      "       -0.01463253,  0.01449414,  0.04426914,  0.06301586], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_129), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_51/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=\n",
      "array([[[[ 0.05166207,  0.        ,  0.        , ...,  0.        ,\n",
      "          -0.05603538,  0.        ],\n",
      "         [-0.08537063,  0.        , -0.06810174, ..., -0.08101234,\n",
      "           0.05948679,  0.        ],\n",
      "         [-0.09115181, -0.05088386,  0.1126333 , ...,  0.        ,\n",
      "          -0.05025968, -0.07059135],\n",
      "         ...,\n",
      "         [ 0.09313451,  0.02339831,  0.        , ...,  0.        ,\n",
      "          -0.05538806,  0.04289646],\n",
      "         [ 0.03124104,  0.03113287, -0.11517435, ...,  0.02399503,\n",
      "           0.03917634,  0.        ],\n",
      "         [ 0.07573703,  0.        ,  0.        , ...,  0.        ,\n",
      "           0.        , -0.04145778]],\n",
      "\n",
      "        [[ 0.04485036,  0.04113337,  0.        , ...,  0.05007312,\n",
      "           0.04784397, -0.05317488],\n",
      "         [ 0.        ,  0.        , -0.09244221, ...,  0.        ,\n",
      "           0.        ,  0.03891414],\n",
      "         [-0.03254365, -0.02995321,  0.02818144, ..., -0.05262243,\n",
      "          -0.04935859, -0.04585725],\n",
      "         ...,\n",
      "         [ 0.10051308,  0.05755199,  0.03334146, ..., -0.07925887,\n",
      "           0.04589071,  0.        ],\n",
      "         [ 0.03141587,  0.        , -0.08028854, ..., -0.06447326,\n",
      "           0.0926927 ,  0.        ],\n",
      "         [-0.03811533,  0.        ,  0.06843925, ..., -0.12218343,\n",
      "           0.07723182,  0.        ]],\n",
      "\n",
      "        [[ 0.03985931,  0.04884269,  0.        , ..., -0.05289852,\n",
      "          -0.02542869,  0.03581015],\n",
      "         [-0.05890944,  0.        , -0.0408016 , ...,  0.        ,\n",
      "          -0.0326796 ,  0.        ],\n",
      "         [ 0.02643568, -0.07701217,  0.09659059, ..., -0.06098502,\n",
      "           0.        ,  0.05827847],\n",
      "         ...,\n",
      "         [ 0.11744472,  0.        , -0.05549761, ..., -0.04801305,\n",
      "          -0.07594927,  0.04623646],\n",
      "         [ 0.0839175 ,  0.04829041, -0.03152424, ...,  0.        ,\n",
      "           0.11369945,  0.03453949],\n",
      "         [ 0.10741381, -0.10472177,  0.10080413, ..., -0.08653181,\n",
      "           0.0408784 , -0.02612746]]],\n",
      "\n",
      "\n",
      "       [[[ 0.04475339, -0.0627856 ,  0.        , ...,  0.04412005,\n",
      "           0.        , -0.06133368],\n",
      "         [-0.06177229,  0.        ,  0.        , ...,  0.        ,\n",
      "           0.03044016,  0.        ],\n",
      "         [ 0.        ,  0.0399546 ,  0.05866556, ..., -0.06938254,\n",
      "          -0.02416775, -0.08029216],\n",
      "         ...,\n",
      "         [ 0.04740028,  0.03805437,  0.04850341, ...,  0.02799934,\n",
      "          -0.03084476,  0.        ],\n",
      "         [-0.02883117,  0.        , -0.09054859, ...,  0.03304573,\n",
      "           0.09680959,  0.06041223],\n",
      "         [ 0.03835399,  0.        ,  0.0253755 , ...,  0.        ,\n",
      "           0.        ,  0.0418939 ]],\n",
      "\n",
      "        [[-0.06013699,  0.        ,  0.04124462, ...,  0.06013044,\n",
      "          -0.02951898,  0.02924094],\n",
      "         [-0.09360842, -0.04140791, -0.10105919, ..., -0.02969278,\n",
      "          -0.04253607,  0.        ],\n",
      "         [-0.02580275, -0.02461049,  0.09087986, ...,  0.        ,\n",
      "           0.04430314,  0.        ],\n",
      "         ...,\n",
      "         [-0.04215175,  0.05439839,  0.        , ...,  0.        ,\n",
      "          -0.05323048,  0.        ],\n",
      "         [ 0.        ,  0.03958659, -0.07180499, ..., -0.03182979,\n",
      "           0.13330127, -0.03217443],\n",
      "         [ 0.        , -0.04892636,  0.03104359, ...,  0.        ,\n",
      "          -0.0633661 ,  0.04108941]],\n",
      "\n",
      "        [[ 0.05097246,  0.05506524,  0.02792042, ...,  0.04354533,\n",
      "          -0.033476  , -0.02730851],\n",
      "         [-0.03911319, -0.03467229, -0.08965371, ..., -0.03282092,\n",
      "           0.02725036, -0.03391355],\n",
      "         [ 0.05004779,  0.        ,  0.        , ...,  0.        ,\n",
      "          -0.03979808,  0.04898906],\n",
      "         ...,\n",
      "         [ 0.11058699, -0.06637526,  0.        , ...,  0.        ,\n",
      "           0.        , -0.06027561],\n",
      "         [ 0.        , -0.04243203, -0.04058589, ..., -0.05187544,\n",
      "           0.        , -0.02703868],\n",
      "         [ 0.        , -0.10347336,  0.0407092 , ...,  0.        ,\n",
      "           0.03714931,  0.        ]]],\n",
      "\n",
      "\n",
      "       [[[-0.09129382,  0.        ,  0.10087536, ...,  0.        ,\n",
      "           0.        ,  0.03874984],\n",
      "         [-0.05556564,  0.06828967,  0.05817242, ...,  0.04637947,\n",
      "           0.        , -0.07849108],\n",
      "         [ 0.05374841,  0.07686673,  0.        , ..., -0.07344831,\n",
      "          -0.06042738,  0.        ],\n",
      "         ...,\n",
      "         [-0.02897346, -0.0660847 , -0.06610155, ..., -0.02976345,\n",
      "           0.05788524,  0.04228853],\n",
      "         [ 0.04603473,  0.04175783, -0.11487645, ...,  0.        ,\n",
      "           0.        ,  0.04538163],\n",
      "         [-0.03632413, -0.06905077,  0.03720974, ...,  0.10498254,\n",
      "           0.03613424,  0.        ]],\n",
      "\n",
      "        [[ 0.03266626,  0.07086556,  0.0903535 , ...,  0.        ,\n",
      "           0.04880555,  0.        ],\n",
      "         [ 0.        ,  0.05123224,  0.        , ...,  0.06240197,\n",
      "           0.        ,  0.        ],\n",
      "         [ 0.09945059,  0.03337682,  0.        , ..., -0.08290748,\n",
      "           0.08333662,  0.0410984 ],\n",
      "         ...,\n",
      "         [ 0.08259038, -0.06432278,  0.        , ...,  0.        ,\n",
      "           0.        ,  0.        ],\n",
      "         [ 0.        , -0.05929597, -0.03036189, ...,  0.        ,\n",
      "           0.06237262,  0.        ],\n",
      "         [ 0.        ,  0.02707377,  0.        , ...,  0.        ,\n",
      "           0.        , -0.042594  ]],\n",
      "\n",
      "        [[-0.03730092,  0.        ,  0.05063778, ..., -0.04227864,\n",
      "           0.09524783,  0.        ],\n",
      "         [ 0.        ,  0.04091522, -0.14045975, ...,  0.06306539,\n",
      "          -0.12370417, -0.03583243],\n",
      "         [-0.03913566, -0.03308174,  0.06264736, ...,  0.        ,\n",
      "           0.11100301,  0.09236067],\n",
      "         ...,\n",
      "         [ 0.13390887, -0.08587841,  0.04293307, ...,  0.        ,\n",
      "           0.03833069,  0.        ],\n",
      "         [ 0.        , -0.08682144,  0.0525591 , ..., -0.11246598,\n",
      "           0.        , -0.10771642],\n",
      "         [ 0.        , -0.08664317,  0.        , ...,  0.09106494,\n",
      "           0.04657073, -0.05659252]]]], dtype=float32)>\n",
      "  <tf.Variable 'conv2d_51/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([ 0.01431534,  0.01007325, -0.0512156 ,  0.00877426,  0.03273134,\n",
      "        0.02648268,  0.01091461, -0.00021867,  0.03370868,  0.03235943,\n",
      "       -0.02969854, -0.00241652,  0.03239983, -0.00243742,  0.0088049 ,\n",
      "        0.04827288,  0.02357249,  0.00800345, -0.0103647 ,  0.01291097,\n",
      "        0.05441098,  0.00692773,  0.02457991,  0.01893401,  0.05718968,\n",
      "        0.04592552, -0.02765448,  0.00296753,  0.04030789,  0.00177203,\n",
      "        0.00183015, -0.00050552,  0.00060599,  0.00785219,  0.00828243,\n",
      "       -0.00678958,  0.01479873,  0.00202317,  0.0144478 ,  0.05244561,\n",
      "        0.01762904,  0.02306481,  0.02197723,  0.00104602,  0.01879616,\n",
      "        0.01460775, -0.00299703,  0.03577923,  0.03732445,  0.03961362,\n",
      "       -0.00337283,  0.02345583,  0.00771783,  0.05988278,  0.04150386,\n",
      "        0.01271234, -0.00705716,  0.00519825,  0.04214123,  0.02987778,\n",
      "       -0.0323905 , -0.00489125,  0.0234437 , -0.03427849], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_130), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dense_26/kernel:0' shape=(1024, 512) dtype=float32, numpy=\n",
      "array([[ 0.04911113,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.07939629,  0.02112951],\n",
      "       [ 0.        ,  0.03731434,  0.04102445, ...,  0.        ,\n",
      "        -0.05947237,  0.        ],\n",
      "       [-0.04811303, -0.06498837, -0.05316801, ...,  0.        ,\n",
      "        -0.04396584,  0.02860425],\n",
      "       ...,\n",
      "       [-0.0442427 , -0.02144182,  0.        , ...,  0.        ,\n",
      "        -0.04645784,  0.        ],\n",
      "       [ 0.10279959,  0.05777968, -0.08722869, ...,  0.07473385,\n",
      "        -0.03234334,  0.07807387],\n",
      "       [ 0.03425081,  0.030875  ,  0.        , ...,  0.        ,\n",
      "        -0.0668156 ,  0.        ]], dtype=float32)>\n",
      "  <tf.Variable 'dense_26/bias:0' shape=(512,) dtype=float32, numpy=\n",
      "array([ 0.04068105,  0.04507537,  0.02572727,  0.00370414,  0.02634983,\n",
      "       -0.0126495 ,  0.006378  ,  0.02903828,  0.03944199,  0.04777186,\n",
      "        0.01645017,  0.03004258, -0.00789591,  0.03420521,  0.05770613,\n",
      "        0.03733688, -0.01766558,  0.01107513,  0.04505277, -0.00016568,\n",
      "       -0.00178825, -0.01842957,  0.03645731,  0.02527924, -0.02335552,\n",
      "        0.01531384,  0.03536025,  0.02130992,  0.02491067,  0.02763184,\n",
      "        0.02125472, -0.00111058, -0.02430911,  0.02800788,  0.02478989,\n",
      "        0.03495685, -0.00823822, -0.00403862,  0.05824598,  0.05322789,\n",
      "        0.02287039,  0.01633305, -0.01219378,  0.01973678,  0.02553892,\n",
      "        0.02199557,  0.01219853,  0.01045095,  0.00714356,  0.00932126,\n",
      "        0.02476909,  0.04497864,  0.04731353, -0.01852106,  0.02306558,\n",
      "        0.05771867, -0.00272454,  0.01691362,  0.00353927,  0.03398967,\n",
      "        0.04005741, -0.00687215, -0.02560554, -0.01259466, -0.01105675,\n",
      "        0.01674816, -0.00301269, -0.0019803 ,  0.03274065, -0.02550539,\n",
      "        0.0040462 ,  0.0122524 ,  0.00812544, -0.02314672, -0.01423356,\n",
      "        0.02122127, -0.00014969, -0.00635583,  0.02038952,  0.03079717,\n",
      "       -0.01741716,  0.00419151,  0.04036739,  0.00788932,  0.01232304,\n",
      "        0.00786792,  0.03729263, -0.01877387, -0.02157894,  0.03762031,\n",
      "       -0.01755287,  0.0382758 ,  0.05086979, -0.01829809,  0.03475393,\n",
      "        0.03353722, -0.01002014, -0.01147911,  0.02374067,  0.01078867,\n",
      "        0.06120225,  0.00324735,  0.03606996, -0.01287301,  0.00382351,\n",
      "        0.02991395,  0.02574594, -0.02350515, -0.02893788, -0.0055589 ,\n",
      "       -0.00863868,  0.04258867, -0.00017411, -0.02286437, -0.01954286,\n",
      "        0.00356323,  0.02769497, -0.01954923,  0.0422092 , -0.02148258,\n",
      "        0.03166025,  0.00991192,  0.00438131, -0.02171093,  0.03203283,\n",
      "       -0.00705301,  0.02505085,  0.02621643,  0.03773432,  0.05714565,\n",
      "       -0.00492823,  0.00557355,  0.01247894,  0.02688801,  0.02879676,\n",
      "        0.00356817,  0.04442475,  0.01231088,  0.023277  ,  0.02544466,\n",
      "        0.01505211, -0.00455246, -0.03224391,  0.02201528,  0.02064391,\n",
      "        0.04201201,  0.04540722, -0.0269284 , -0.01196724,  0.02767679,\n",
      "        0.03320676,  0.03155752, -0.00638557, -0.0185382 ,  0.03171559,\n",
      "        0.021067  ,  0.03299935,  0.02368706,  0.03152472,  0.02743309,\n",
      "        0.01112669,  0.03783202,  0.0570345 ,  0.03119624, -0.02903889,\n",
      "        0.00015125,  0.0222578 , -0.00023994,  0.01709589,  0.02404158,\n",
      "        0.01717468,  0.01024964, -0.01524673,  0.04030381,  0.04632984,\n",
      "        0.00991998,  0.04987736,  0.044487  , -0.01261663,  0.01984622,\n",
      "        0.03409293,  0.0417784 ,  0.01071721, -0.00185445, -0.01299766,\n",
      "       -0.0158341 ,  0.00732208, -0.00653082, -0.01123167,  0.02702255,\n",
      "        0.02403366, -0.01645791,  0.01417051,  0.01774074,  0.03629282,\n",
      "       -0.02157143, -0.00084323,  0.0211419 , -0.03080962,  0.02564111,\n",
      "        0.04512368,  0.03446762,  0.02774893, -0.01936135, -0.00114642,\n",
      "        0.02750787,  0.04003875, -0.00819566,  0.01321722, -0.00940653,\n",
      "        0.04020164,  0.02801715,  0.01502398, -0.02848581, -0.01853932,\n",
      "        0.00622284,  0.03640723,  0.00814249, -0.01367826, -0.00378064,\n",
      "        0.01033575, -0.02349707,  0.01676702,  0.02779622, -0.01366777,\n",
      "        0.00469944,  0.00303701,  0.00296351, -0.0018102 ,  0.01913166,\n",
      "       -0.02635701, -0.03099236, -0.00132457,  0.05682323,  0.02553881,\n",
      "        0.00804083,  0.0131645 ,  0.01487579,  0.02488204,  0.04347596,\n",
      "        0.01852134, -0.01989945, -0.0071291 , -0.01323157,  0.00347451,\n",
      "        0.01375578,  0.04942763, -0.02208079,  0.00340076,  0.0263248 ,\n",
      "       -0.00654788, -0.00189484,  0.02989372,  0.04400297, -0.01431515,\n",
      "        0.02895118, -0.02206763,  0.02495473, -0.00602242,  0.03487289,\n",
      "       -0.01689127,  0.0068549 ,  0.0288748 , -0.02833848,  0.02791514,\n",
      "        0.04531341, -0.01571045,  0.01127758,  0.02985667, -0.03361287,\n",
      "       -0.03008919, -0.0288597 ,  0.02651928, -0.02855921, -0.02435813,\n",
      "        0.03214438,  0.01517075,  0.0474641 ,  0.03806872, -0.00305494,\n",
      "       -0.00982911, -0.00974104,  0.03282562,  0.00337498,  0.023673  ,\n",
      "       -0.00088877,  0.02922083,  0.01364733,  0.04646626,  0.02191222,\n",
      "        0.00543124,  0.02098679,  0.02185004,  0.02762446,  0.00807869,\n",
      "        0.02271981, -0.01533973,  0.02224228,  0.01547489,  0.00992205,\n",
      "        0.01441119,  0.00543532, -0.00292549,  0.02659933,  0.01255483,\n",
      "       -0.00370667,  0.00583162,  0.03049396,  0.00218846,  0.0304812 ,\n",
      "        0.0384296 ,  0.00786863,  0.04123438,  0.03028365,  0.01791007,\n",
      "       -0.00133049,  0.02572794,  0.01758247,  0.01333413,  0.02464796,\n",
      "        0.00128365,  0.04545245,  0.00305977, -0.02080398, -0.00701179,\n",
      "        0.05419271,  0.00673707,  0.01849206, -0.01728289, -0.01754375,\n",
      "        0.03903135,  0.01884096, -0.00053122, -0.00208695,  0.00642356,\n",
      "        0.01419606, -0.00843431,  0.03298777,  0.02552282,  0.04412488,\n",
      "       -0.00721649, -0.00430154, -0.03211602,  0.02224624,  0.02620199,\n",
      "       -0.01530993, -0.01529911, -0.02616035,  0.03386283,  0.0182275 ,\n",
      "        0.00757066, -0.01460111, -0.00358231,  0.01613697, -0.01318344,\n",
      "       -0.00577025,  0.01444518,  0.01078408,  0.0058286 , -0.02030956,\n",
      "        0.00865928, -0.02194248,  0.01572553,  0.01273498,  0.04316321,\n",
      "        0.04958766,  0.01120074,  0.04830822,  0.04388975, -0.01265839,\n",
      "       -0.02594121,  0.06880338,  0.02869133,  0.01787707,  0.0125932 ,\n",
      "        0.03395972,  0.02024298,  0.00707471,  0.01656832,  0.04030023,\n",
      "        0.06018287, -0.03034824,  0.01712391,  0.03160473, -0.00045795,\n",
      "        0.04691555,  0.00915789,  0.02800056,  0.03171456,  0.01165651,\n",
      "        0.04766122,  0.05116891, -0.0120165 ,  0.03467623,  0.04376807,\n",
      "        0.04525973,  0.02483006,  0.02377692, -0.00205516,  0.00725909,\n",
      "       -0.00368159,  0.00236644, -0.02578513,  0.05096506,  0.01390264,\n",
      "       -0.03305891,  0.0413697 , -0.00441925,  0.00325452, -0.01275964,\n",
      "        0.01038716,  0.03073081,  0.03565152,  0.00208933,  0.01623733,\n",
      "        0.01113926,  0.03437103, -0.00746084, -0.0076894 , -0.02700642,\n",
      "       -0.00100564, -0.00876176, -0.01267159,  0.03545088,  0.02720273,\n",
      "        0.04415079,  0.01342897,  0.03057099,  0.00322271,  0.00060333,\n",
      "        0.02716102,  0.01219019,  0.03042564,  0.02694482, -0.00630283,\n",
      "        0.00857954,  0.01761556, -0.00545551,  0.02349447, -0.01058566,\n",
      "        0.01815249, -0.01817088,  0.01707341,  0.02754847, -0.00591227,\n",
      "        0.01613345,  0.01813663,  0.02812108,  0.04539419,  0.01121944,\n",
      "       -0.00055684,  0.0145818 ,  0.03222806,  0.04926399,  0.02324511,\n",
      "        0.04004991, -0.00500092, -0.00155115,  0.04103447,  0.04515169,\n",
      "        0.00395099, -0.01455406,  0.00206286,  0.03025736,  0.0269616 ,\n",
      "        0.02723145,  0.04178732,  0.03782218,  0.01761984,  0.04672964,\n",
      "        0.02193571, -0.00751976,  0.03584972,  0.01044406, -0.02024456,\n",
      "        0.01883595, -0.01297722,  0.04939317, -0.00380289,  0.01890155,\n",
      "        0.01889993,  0.03929559, -0.01057986, -0.01152092,  0.00782449,\n",
      "        0.00894414,  0.01173238,  0.02349932,  0.03469514,  0.03854604,\n",
      "        0.01666903,  0.01855629,  0.0345252 ,  0.03693186,  0.03727915,\n",
      "        0.05205087,  0.03543804,  0.01531635,  0.00977384,  0.02488458,\n",
      "        0.04446049,  0.02108902, -0.02771337,  0.03582015,  0.02324297,\n",
      "        0.0399452 ,  0.01958478, -0.0215325 ,  0.04535371,  0.02666425,\n",
      "        0.00870305, -0.00824767], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_131), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dense_27/kernel:0' shape=(512, 5) dtype=float32, numpy=\n",
      "array([[ 0.08866224,  0.06363897,  0.0796665 , -0.20396852,  0.09184968],\n",
      "       [ 0.        ,  0.        ,  0.        , -0.2381756 ,  0.09391141],\n",
      "       [-0.176418  ,  0.        ,  0.14408004,  0.        , -0.09630208],\n",
      "       ...,\n",
      "       [ 0.        ,  0.14729083,  0.09802785, -0.1460184 , -0.10773033],\n",
      "       [-0.17310837,  0.20419706,  0.13516541,  0.        ,  0.        ],\n",
      "       [ 0.20743962,  0.        , -0.09734035, -0.20471908,  0.        ]],\n",
      "      dtype=float32)>\n",
      "  <tf.Variable 'dense_27/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.06145976,  0.04942695,  0.06626821, -0.05692418,  0.0045718 ],\n",
      "      dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703/703 [==============================] - 47s 67ms/step - loss: 0.8742 - accuracy: 0.6682 - val_loss: 0.9359 - val_accuracy: 0.6467\n"
     ]
    }
   ],
   "source": [
    "# you can use the default hyper-parameters for training, \n",
    "# and val accuracy ~59% after 25 epochs and > 63% after 50 epochs\n",
    "\n",
    "pruned_model.compile(run_eagerly=True, optimizer=keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 1\n",
    "history = pruned_model.fit(train_images, train_labels, batch_size=32, epochs=epochs, \n",
    "                    validation_data=(val_images, val_labels)) # train for 50 epochs, with batch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaNN-67oLlVJ",
    "outputId": "88a50217-97ba-48d3-c6cc-f9e2698bc4ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_126 (Lambda)         multiple                  0         \n",
      "                                                                 \n",
      " activation_78 (Activation)  (None, 25, 25, 32)        0         \n",
      "                                                                 \n",
      " lambda_127 (Lambda)         multiple                  0         \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 23, 23, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 11, 11, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 11, 11, 32)        0         \n",
      "                                                                 \n",
      " lambda_128 (Lambda)         multiple                  0         \n",
      "                                                                 \n",
      " activation_80 (Activation)  (None, 11, 11, 64)        0         \n",
      "                                                                 \n",
      " lambda_129 (Lambda)         multiple                  0         \n",
      "                                                                 \n",
      " activation_81 (Activation)  (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " lambda_130 (Lambda)         multiple                  0         \n",
      "                                                                 \n",
      " activation_82 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " lambda_131 (Lambda)         multiple                  0         \n",
      "                                                                 \n",
      " activation_83 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pruned_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "wMSKQW4k4T-G",
    "outputId": "3acb789e-4177-42ba-a9d1-6f57c80dd05f"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_a77bd0e0-8120-4bba-abdc-877e6f807e53\", \"my_model_weights_1.h5\", 2407560)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you need to save the model's weights, naming it 'my_model_weights.h5'\n",
    "model.save_weights(\"my_model_weights_1.h5\")\n",
    "\n",
    "# running this cell will immediately download a file called 'my_model_weights.h5'\n",
    "from google.colab import files\n",
    "files.download(\"my_model_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEXWJJovJ-f9",
    "outputId": "868dccde-95fb-4b09-f718-12037f15b38a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 56ms/step - loss: 0.9359 - accuracy: 0.6467\n"
     ]
    }
   ],
   "source": [
    "results = pruned_model.evaluate(val_images, val_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_CUKxYOKCh8",
    "outputId": "d52f8a07-1d2a-4b3a-e15b-847adbe42906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29964262404015296\n"
     ]
    }
   ],
   "source": [
    "# perform pruning here\n",
    "\n",
    "# get the weights \n",
    "# weights = pruned_model.get_weights()\n",
    "weights = model.get_weights()\n",
    "\n",
    "# you can use set_weights() to set some weights to zero, e.g.,\n",
    "# some references for pruning techniques: https://arxiv.org/pdf/1810.05270v2.pdf, https://arxiv.org/pdf/2001.04062.pdf\n",
    "\n",
    "# model.set_weights(weights)\n",
    "# print(weights)\n",
    "\n",
    "global zero_nums\n",
    "zero_nums = 0\n",
    "def compute(arr):\n",
    "  if hasattr(arr, '__len__'):\n",
    "    for i in arr:\n",
    "      compute(i)\n",
    "  else:\n",
    "    # print(type(arr))\n",
    "    if arr == 0:\n",
    "      global zero_nums\n",
    "      zero_nums += 1\n",
    "\n",
    "\n",
    "nums = 592933\n",
    "compute(weights)\n",
    "# # zero_nums = 0\n",
    "# for w in weights:\n",
    "#   # nums += len(w)\n",
    "#   for i in w:\n",
    "#     if i == 0:\n",
    "#       zero_nums += 1\n",
    "\n",
    "print(zero_nums / nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o4VSZBH-d3sZ"
   },
   "outputs": [],
   "source": [
    "# Define the neural network architecture (don't change this)\n",
    "class CustomizedConv2D(Conv2D):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(CustomizedConv2D, self).__init__(*args, **kwargs)\n",
    "\n",
    "model = models.Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n",
    "model.add(CustomizedConv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(CustomizedConv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(CustomizedConv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(CustomizedConv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
